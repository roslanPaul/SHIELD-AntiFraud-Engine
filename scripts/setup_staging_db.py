# scripts/setup_staging_db.py
"""
Script de crÃ©ation de l'environnement de staging SQL.

Transforme les fichiers CSV bruts en base de donnÃ©es relationnelle
avec index, contraintes et mÃ©tadonnÃ©es.

Usage:
    python scripts/setup_staging_db.py
"""

import pandas as pd
import sqlite3
import logging
from pathlib import Path
import sys
from datetime import datetime

# Ajouter le dossier parent au path
sys.path.append(str(Path(__file__).parent.parent))
from config import DATA_DIR, DATABASE_PATH, TABLES, LOG_LEVEL, LOG_FORMAT

# Configuration logging
logging.basicConfig(level=LOG_LEVEL, format=LOG_FORMAT)
logger = logging.getLogger(__name__)


class StagingDatabaseSetup:
    """
    Gestionnaire de crÃ©ation de la base de donnÃ©es de staging.
    
    FonctionnalitÃ©s :
    - Import des CSV
    - CrÃ©ation des index
    - Validation des donnÃ©es
    - GÃ©nÃ©ration de statistiques
    """
    
    def __init__(self, data_dir: Path = DATA_DIR, db_path: Path = DATABASE_PATH):
        self.data_dir = data_dir
        self.db_path = db_path
        self.conn = None
        self.stats = {}
    
    def connect(self):
        """Ã‰tablit la connexion Ã  la base."""
        logger.info(f"ğŸ“‚ Connexion Ã  {self.db_path}")
        self.conn = sqlite3.connect(self.db_path)
        self.conn.execute("PRAGMA foreign_keys = ON")  # Activer les contraintes FK
    
    def import_table(self, table_name: str, csv_filename: str) -> bool:
        """
        Import une table depuis un CSV.
        
        Args:
            table_name: Nom de la table SQL
            csv_filename: Nom du fichier CSV
        
        Returns:
            True si succÃ¨s
        """
        csv_path = self.data_dir / csv_filename
        
        if not csv_path.exists():
            logger.error(f"âŒ Fichier introuvable : {csv_path}")
            return False
        
        try:
            logger.info(f"ğŸ“¥ Import de {table_name}...")
            
            # Lecture CSV
            df = pd.read_csv(csv_path)
            
            # Import dans SQLite
            df.to_sql(table_name, self.conn, if_exists='replace', index=False)
            
            # Statistiques
            self.stats[table_name] = {
                'rows': len(df),
                'columns': len(df.columns),
                'size_mb': csv_path.stat().st_size / (1024 * 1024)
            }
            
            logger.info(f"   âœ… {len(df):,} lignes importÃ©es ({len(df.columns)} colonnes)")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Erreur import {table_name} : {e}")
            return False
    
    def create_indexes(self):
        """
        CrÃ©e les index pour optimiser les performances des requÃªtes.
        
        Best Practice : Index sur les clÃ©s Ã©trangÃ¨res et colonnes frÃ©quemment filtrÃ©es
        """
        logger.info("ğŸ”§ CrÃ©ation des index...")
        
        indexes = [
            # ClÃ©s primaires
            "CREATE INDEX IF NOT EXISTS idx_customer_id ON customer_profile(customer_id)",
            "CREATE INDEX IF NOT EXISTS idx_merchant_id ON merchant_registry(merchant_id)",
            "CREATE INDEX IF NOT EXISTS idx_transaction_id ON transactions(transaction_id)",
            "CREATE INDEX IF NOT EXISTS idx_device_txn ON device_fingerprinting(transaction_id)",
            "CREATE INDEX IF NOT EXISTS idx_alert_txn ON fraud_alerts_history(transaction_id)",
            
            # ClÃ©s Ã©trangÃ¨res (pour JOINs)
            "CREATE INDEX IF NOT EXISTS idx_txn_customer ON transactions(customer_id)",
            "CREATE INDEX IF NOT EXISTS idx_txn_merchant ON transactions(merchant_id)",
            "CREATE INDEX IF NOT EXISTS idx_alert_customer ON fraud_alerts_history(customer_id)",
            
            # Colonnes de filtrage frÃ©quent
            "CREATE INDEX IF NOT EXISTS idx_txn_timestamp ON transactions(transaction_timestamp)",
            "CREATE INDEX IF NOT EXISTS idx_txn_fraud ON transactions(is_fraud)",
            "CREATE INDEX IF NOT EXISTS idx_merchant_country ON merchant_registry(merchant_country)",
            "CREATE INDEX IF NOT EXISTS idx_alert_confirmed ON fraud_alerts_history(is_confirmed_fraud)",
            "CREATE INDEX IF NOT EXISTS idx_device_vpn ON device_fingerprinting(is_vpn)",
        ]
        
        for idx_query in indexes:
            try:
                self.conn.execute(idx_query)
                logger.info(f"   âœ… {idx_query.split('idx_')[1].split(' ')[0]}")
            except Exception as e:
                logger.warning(f"   âš ï¸  Index dÃ©jÃ  existant ou erreur : {e}")
        
        self.conn.commit()
        logger.info("âœ… Index crÃ©Ã©s")
    
    def validate_data_quality(self):
        """
        Valide la qualitÃ© des donnÃ©es importÃ©es.
        
        Checks :
        - Pas de NULL dans les clÃ©s primaires
        - CohÃ©rence des foreign keys
        - Distribution des labels
        """
        logger.info("ğŸ” Validation de la qualitÃ© des donnÃ©es...")
        
        # Check 1 : ClÃ©s primaires uniques
        checks = [
            ("UnicitÃ© customer_id", 
             "SELECT COUNT(*) - COUNT(DISTINCT customer_id) as dups FROM customer_profile"),
            
            ("UnicitÃ© merchant_id", 
             "SELECT COUNT(*) - COUNT(DISTINCT merchant_id) as dups FROM merchant_registry"),
            
            ("UnicitÃ© transaction_id", 
             "SELECT COUNT(*) - COUNT(DISTINCT transaction_id) as dups FROM transactions"),
            
            # Check 2 : Foreign keys
            ("CohÃ©rence FK customer", 
             """SELECT COUNT(*) as orphans 
                FROM transactions t 
                LEFT JOIN customer_profile c ON t.customer_id = c.customer_id 
                WHERE c.customer_id IS NULL"""),
            
            ("CohÃ©rence FK merchant", 
             """SELECT COUNT(*) as orphans 
                FROM transactions t 
                LEFT JOIN merchant_registry m ON t.merchant_id = m.merchant_id 
                WHERE m.merchant_id IS NULL"""),
            
            # Check 3 : Distribution labels
            ("Taux de fraude", 
             "SELECT AVG(is_fraud) * 100 as fraud_pct FROM transactions"),
        ]
        
        all_valid = True
        for check_name, query in checks:
            try:
                cursor = self.conn.execute(query)
                result = cursor.fetchone()[0]
                
                if 'dups' in query or 'orphans' in query:
                    if result == 0:
                        logger.info(f"   âœ… {check_name} : OK")
                    else:
                        logger.error(f"   âŒ {check_name} : {result} problÃ¨mes")
                        all_valid = False
                else:
                    logger.info(f"   â„¹ï¸  {check_name} : {result:.3f}%")
            except Exception as e:
                logger.error(f"   âŒ Erreur check {check_name} : {e}")
                all_valid = False
        
        return all_valid
    
    def generate_metadata(self):
        """
        GÃ©nÃ¨re une table de mÃ©tadonnÃ©es pour documentation.
        
        Best Practice : Toujours documenter la provenance des donnÃ©es
        """
        logger.info("ğŸ“ GÃ©nÃ©ration des mÃ©tadonnÃ©es...")
        
        metadata = {
            'table_name': [],
            'row_count': [],
            'column_count': [],
            'size_mb': [],
            'created_at': []
        }
        
        for table_name, stats in self.stats.items():
            metadata['table_name'].append(table_name)
            metadata['row_count'].append(stats['rows'])
            metadata['column_count'].append(stats['columns'])
            metadata['size_mb'].append(round(stats['size_mb'], 2))
            metadata['created_at'].append(datetime.now().isoformat())
        
        df_metadata = pd.DataFrame(metadata)
        df_metadata.to_sql('_metadata', self.conn, if_exists='replace', index=False)
        
        logger.info("âœ… MÃ©tadonnÃ©es sauvegardÃ©es dans table '_metadata'")
    
    def print_summary(self):
        """Affiche un rÃ©sumÃ© de l'environnement crÃ©Ã©."""
        print("\n" + "="*70)
        print("ğŸ“Š RÃ‰SUMÃ‰ DE L'ENVIRONNEMENT DE STAGING")
        print("="*70)
        print(f"\nğŸ“‚ Base de donnÃ©es : {self.db_path}")
        print(f"ğŸ“… Date de crÃ©ation : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"\nğŸ“‹ Tables importÃ©es :")
        
        total_rows = 0
        total_size = 0
        
        for table_name, stats in self.stats.items():
            print(f"   â€¢ {table_name:30s} : {stats['rows']:>10,} lignes  |  {stats['columns']:>2} colonnes  |  {stats['size_mb']:>6.2f} MB")
            total_rows += stats['rows']
            total_size += stats['size_mb']
        
        print(f"\nğŸ’¾ Total : {total_rows:,} lignes | {total_size:.2f} MB")
        print("="*70 + "\n")
    
    def setup(self):
        """Pipeline complet de setup."""
        logger.info("ğŸš€ DÃ©marrage du setup de l'environnement de staging...")
        
        # 1. Connexion
        self.connect()
        
        # 2. Import des tables
        all_imported = True
        for table_name, csv_filename in TABLES.items():
            if not self.import_table(table_name, csv_filename):
                all_imported = False
        
        if not all_imported:
            logger.error("âŒ Certaines tables n'ont pas pu Ãªtre importÃ©es")
            return False
        
        # 3. CrÃ©ation des index
        self.create_indexes()
        
        # 4. Validation
        if not self.validate_data_quality():
            logger.warning("âš ï¸  Certains checks de qualitÃ© ont Ã©chouÃ©")
        
        # 5. MÃ©tadonnÃ©es
        self.generate_metadata()
        
        # 6. RÃ©sumÃ©
        self.print_summary()
        
        # 7. Fermeture
        self.conn.close()
        
        logger.info("âœ… Setup terminÃ© avec succÃ¨s")
        return True


def main():
    """Point d'entrÃ©e principal."""
    print("\n" + "="*70)
    print("ğŸ—ï¸  SETUP ENVIRONNEMENT DE STAGING - PROJET SHIELD")
    print("="*70 + "\n")
    
    # VÃ©rification que les CSV existent
    if not DATA_DIR.exists():
        logger.error(f"âŒ Dossier {DATA_DIR} introuvable")
        logger.info("ğŸ’¡ Conseil : ExÃ©cuter d'abord le simulateur de donnÃ©es")
        return
    
    # Setup
    setup = StagingDatabaseSetup()
    success = setup.setup()
    
    if success:
        print("\nâœ… Environnement prÃªt ! Tu peux maintenant :")
        print("   1. Lancer le notebook d'analyse : notebooks/01_EDA_Risk_Analysis.ipynb")
        print("   2. Tester la connexion : python scripts/db_connection.py")
        print("   3. ExÃ©cuter des requÃªtes SQL depuis : sql/")
    else:
        print("\nâŒ Le setup a Ã©chouÃ© - VÃ©rifier les logs ci-dessus")


if __name__ == "__main__":
    main()
